{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parte introduttiva:\n",
    " - configurazione\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' python -m spacy download en_core_web_sm '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Installazione delle librerie tramite conda\n",
    "\"\"\" conda install -c conda-forge pytesseract pdf2image opencv pillow spacy pandas \"\"\"\n",
    "# Installazione delle librerie tramite pip\n",
    "\"\"\" pip install pdfplumber transformers torch tensorflow \"\"\"\n",
    "# Scarica il modello di SpaCy:\n",
    "\"\"\" python -m spacy download en_core_web_sm \"\"\"\n",
    "\n",
    "# TUTORIAL INSTALLAZIONE E CONFIGURAZIONE tessseract odcr: https://www.youtube.com/watch?v=GMMZAddRxs8&ab_channel=DataSlinger\n",
    "# Cos'è Tesseract OCR??\n",
    "# Tesseract è un motore OCR open source sviluppato da Google, in grado di riconoscere il testo in immagini e PDF.\n",
    "\n",
    "# Per camelot:\n",
    "# conda install -c conda-forge camelot-py\n",
    "\n",
    "# Cos'è SpaCy?? \n",
    "# https://spacy.io/models , libreira funzioni NLP (I MODULI DI SPACY vanno instalalti, esempio python -m spacy download it_core_news_lg):\n",
    "\n",
    "# Caratteristiche Principali\n",
    "# Tokenizzazione: SpaCy può suddividere un testo in parole, punteggiatura e altri elementi (token).\n",
    "# Part-of-Speech Tagging (POS): Assegna una categoria grammaticale (nome, verbo, aggettivo, ecc.) a ciascun token.\n",
    "# Dependency Parsing: Analizza le relazioni grammaticali tra parole in una frase.\n",
    "# Named Entity Recognition (NER): Riconosce e classifica entità nominate (come persone, organizzazioni, luoghi, date) nel testo.\n",
    "# Lemmatizzazione: Riduce le parole alle loro forme base o radici (lemma).\n",
    "# Supporto per più lingue: SpaCy supporta molte lingue diverse e include modelli pre-addestrati per diverse lingue.\n",
    "# Vettori di parole: Supporta modelli di word embedding come Word2Vec, GloVe e FastText per rappresentare parole in spazi vettoriali continui.\n",
    "# Pipeline personalizzabili: Gli utenti possono creare pipeline di elaborazione personalizzate e aggiungere i propri componenti.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- idea finale, obbiettivo da perseguire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA ALLA BASE: https://medium.com/analytics-vidhya/invoice-information-extraction-using-ocr-and-deep-learning-b79464f54d69\n",
    "# TUTORIAL CHE STO SEGUENDO PER ADDESTRARE LayoutLM (va addestrato su un DATASET custom):\n",
    "#  --->  https://youtu.be/bBwDTY38X58?si=sgFvmpoO_x9JlSAi  <----\n",
    "# Serie di 3 parti scritta come il video: https://medium.com/@shivarama/layoutlmv3-from-zero-to-hero-part-1-85d05818eec4\n",
    "# Articolo ottimo: https://arxiv.org/abs/2204.08387\n",
    "# LayoutLMv3 Hugging Face: https://huggingface.co/docs/transformers/model_doc/layoutlmv3\n",
    "# TESSERACT OCR E Label Studio: https://labelstud.io/blog/improve-ocr-quality-for-receipt-processing-with-tesseract-and-label-studio/\n",
    "\n",
    "# Altri link utili:\n",
    "# Classificare documenti con layoutlm: https://www.mlexpert.io/blog/document-classification-with-layoutlmv3\n",
    "# Teoria su huggingface: https://huggingface.co/blog/document-ai\n",
    "\n",
    "# Nota --> vale la pena approfondire anche Donut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pdf2image\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import camelot\n",
    "\n",
    "# Specificare il percorso di Tesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Users/TirocinioMatteo/AppData/Local/Programs/Tesseract-OCR/tesseract.exe'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Esempio utilizzo Tesseract OCR (non usato nei codici sotto--> da provare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Beps\n",
      "A, 10 MINUTI\n",
      "\n",
      "FELNUOVO DEL GARDA\n",
      "Va Miano 129\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imposta il percorso dell'eseguibile di Tesseract (fatto sopra)\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:/Users/TirocinioMatteo/AppData/Local/Programs/Tesseract-OCR/tesseract.exe'  \n",
    "\n",
    "# Carica un'immagine\n",
    "image = Image.open('C:/Users/TirocinioMatteo/Desktop/Matteo/Python_3/Vai/ai/PDF/input_doc/Immagine_cartellone.jpg')\n",
    "\n",
    "# Utilizza Tesseract per estrarre il testo dall'immagine\n",
    "text = pytesseract.image_to_string(image, lang='eng+ita')\n",
    "\n",
    "# Stampa il testo estratto\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Esempio modello NLP SpyCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizzazione:\n",
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n",
      "\n",
      "POS Tagging e Lemmatizzazione:\n",
      "Apple (PROPN): Apple\n",
      "is (AUX): be\n",
      "looking (VERB): look\n",
      "at (ADP): at\n",
      "buying (VERB): buy\n",
      "U.K. (PROPN): U.K.\n",
      "startup (NOUN): startup\n",
      "for (ADP): for\n",
      "$ (SYM): $\n",
      "1 (NUM): 1\n",
      "billion (NUM): billion\n",
      "\n",
      "Named Entity Recognition:\n",
      "Apple (ORG)\n",
      "U.K. (GPE)\n",
      "$1 billion (MONEY)\n",
      "\n",
      "Dependency Parsing:\n",
      "Apple -> nsubj -> looking\n",
      "is -> aux -> looking\n",
      "looking -> ROOT -> looking\n",
      "at -> prep -> looking\n",
      "buying -> pcomp -> at\n",
      "U.K. -> dobj -> buying\n",
      "startup -> dep -> looking\n",
      "for -> prep -> startup\n",
      "$ -> quantmod -> billion\n",
      "1 -> compound -> billion\n",
      "billion -> pobj -> for\n"
     ]
    }
   ],
   "source": [
    "# Esempio applicazioni modello spaCy:https://spacy.io/models\n",
    "\n",
    "# Carica il modello NLP \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Analizza il testo\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Tokenizzazione\n",
    "print(\"Tokenizzazione:\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "# Part-of-Speech Tagging e Lemmatizzazione\n",
    "print(\"\\nPOS Tagging e Lemmatizzazione:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} ({token.pos_}): {token.lemma_}\")\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "print(\"\\nNamed Entity Recognition:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")\n",
    "\n",
    "# Dependency Parsing\n",
    "print(\"\\nDependency Parsing:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Esempio utilizzo Camelot per tabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovate 4 tabelle\n",
      "\n",
      "Tabella 1:\n",
      "                                                   0                 1  2\n",
      "0                                    Gentile Cliente  MATTEO AMAGLIANI  ,\n",
      "1  di seguito sono riportati i movimenti effettua...                     \n",
      "2   informativo e non rappresenta un estratto conto.                     \n",
      "3  I  movimenti  dove  non  è  indicata  una  dat...                     \n",
      "4  pagamento è stato bloccato in seguito alla tua...                     \n",
      "5                              parte dell'esercente.                     \n",
      "6                                    Lista movimenti                     \n",
      "7                       dal 01/03/2023 al 31/03/2023                     \n",
      "\n",
      "Tabella 2:\n",
      "             0           1          2                     3  \\\n",
      "0         Data        Data  Tipologia                  Nome   \n",
      "1   operazione   contabile                                    \n",
      "2   30/03/2023  02/04/2023  Pagamento               Jamaica   \n",
      "3   29/03/2023  31/03/2023  Pagamento  Aeroporto g. Marconi   \n",
      "4                                                             \n",
      "5                                                             \n",
      "6   29/03/2023  31/03/2023  Pagamento            Mcdonald's   \n",
      "7                                                             \n",
      "8                                                             \n",
      "9   28/03/2023  30/03/2023  Pagamento                Unplug   \n",
      "10  27/03/2023  29/03/2023  Pagamento     Mercadona el Clot   \n",
      "11                                                            \n",
      "12                                                            \n",
      "13  26/03/2023  28/03/2023  Pagamento               Ryanair   \n",
      "14  22/03/2023  24/03/2023  Pagamento     Mercadona el Clot   \n",
      "15                                                            \n",
      "16                                                            \n",
      "17  21/03/2023  23/03/2023  Pagamento          Llicencia 74   \n",
      "18  18/03/2023  21/03/2023  Pagamento             Porco dio   \n",
      "19  17/03/2023  19/03/2023  Pagamento      Alliance Vending   \n",
      "20                                                            \n",
      "21                                                            \n",
      "\n",
      "                             4         5  \n",
      "0                  Descrizione   Importo  \n",
      "1                                         \n",
      "2               JAMAICA ANCONA   - 18,3€  \n",
      "3         AEROPORTO G. MARCONI            \n",
      "4                                  - 10€  \n",
      "5                      BOLOGNA            \n",
      "6         MCDONALDS AEROPUERTO            \n",
      "7                                 - 1,7€  \n",
      "8              P EL PRAT DE LL            \n",
      "9                UNPLUG LLEIDA    - 1,3€  \n",
      "10           MERCADONA EL CLOT            \n",
      "11                                - 3,1€  \n",
      "12                      LLEIDA            \n",
      "13         RYANAIR 35314369001  - 57,46€  \n",
      "14           MERCADONA EL CLOT            \n",
      "15                              - 20,34€  \n",
      "16                      LLEIDA            \n",
      "17  LLICENCIA 74 LERIDA LLEIDA   - 9,34€  \n",
      "18            PORCO DIO LLEIDA   - 35,1€  \n",
      "19            ALLIANCE VENDING            \n",
      "20                                - 0,8€  \n",
      "21                   BARCELONA            \n",
      "\n",
      "Tabella 3:\n",
      "             0           1          2                    3  \\\n",
      "0         Data        Data  Tipologia                 Nome   \n",
      "1   operazione   contabile                                   \n",
      "2   15/03/2023  17/03/2023  Pagamento    Mercadona el Clot   \n",
      "3                                                            \n",
      "4                                                            \n",
      "5   15/03/2023  18/03/2023  Pagamento     Alliance Vending   \n",
      "6                                                            \n",
      "7                                                            \n",
      "8   13/03/2023  16/03/2023  Pagamento     Alliance Vending   \n",
      "9                                                            \n",
      "10                                                           \n",
      "11  13/03/2023  16/03/2023  Pagamento     Alliance Vending   \n",
      "12                                                           \n",
      "13                                                           \n",
      "14  11/03/2023  13/03/2023  Pagamento    Mercadona el Clot   \n",
      "15                                                           \n",
      "16                                                           \n",
      "17  10/03/2023  12/03/2023  Pagamento  Beat Cafe i Soul sl   \n",
      "18                                                           \n",
      "19                                                           \n",
      "20  08/03/2023  10/03/2023  Pagamento           la Capital   \n",
      "21  05/03/2023  12/03/2023  Pagamento               Amazon   \n",
      "22                                                           \n",
      "23                                                           \n",
      "24  05/03/2023  07/03/2023  Pagamento                  lit   \n",
      "25  04/03/2023  06/03/2023  Pagamento                Nanni   \n",
      "\n",
      "                         4         5  \n",
      "0              Descrizione   Importo  \n",
      "1                                     \n",
      "2        MERCADONA EL CLOT            \n",
      "3                           - 52,86€  \n",
      "4                   LLEIDA            \n",
      "5         ALLIANCE VENDING            \n",
      "6                             - 0,8€  \n",
      "7                BARCELONA            \n",
      "8         ALLIANCE VENDING            \n",
      "9                             - 0,8€  \n",
      "10               BARCELONA            \n",
      "11        ALLIANCE VENDING            \n",
      "12                            - 0,8€  \n",
      "13               BARCELONA            \n",
      "14       MERCADONA EL CLOT            \n",
      "15                          - 31,47€  \n",
      "16                  LLEIDA            \n",
      "17     BEAT CAFE I SOUL SL            \n",
      "18                           - 21,4€  \n",
      "19                  LLEIDA            \n",
      "20       LA CAPITAL LLEIDA    - 2,6€  \n",
      "21  AMZN MKTP ES AMAZON.ES            \n",
      "22                          - 30,29€  \n",
      "23              LUXEMBOURG            \n",
      "24              LIT LERIDA      - 6€  \n",
      "25         NANNI BARCELONA    - 7,5€  \n",
      "\n",
      "Tabella 4:\n",
      "             0           1          2                       3  \\\n",
      "0         Data        Data  Tipologia                    Nome   \n",
      "1   operazione   contabile                                      \n",
      "2   04/03/2023  06/03/2023  Pagamento                  Zurich   \n",
      "3   04/03/2023  07/03/2023  Pagamento                 Primark   \n",
      "4                                                               \n",
      "5                                                               \n",
      "6   04/03/2023  06/03/2023  Pagamento  Mercadona Consell de c   \n",
      "7                                                               \n",
      "8                                                               \n",
      "9   03/03/2023  05/03/2023  Pagamento       Mercadona el Clot   \n",
      "10                                                              \n",
      "11                                                              \n",
      "12  01/03/2023  03/03/2023  Pagamento       Mercadona el Clot   \n",
      "13                                                              \n",
      "14                                                              \n",
      "15  01/03/2023  03/03/2023  Pagamento              Plus Fresc   \n",
      "16  01/03/2023  03/03/2023  Pagamento                Barberia   \n",
      "17  01/03/2023  01/03/2023  Pagamento     Canone Hype Plus 02   \n",
      "18                                                              \n",
      "19                                                      /2023   \n",
      "20  01/03/2023  04/03/2023  Pagamento              Trenitalia   \n",
      "21                                                              \n",
      "22                                                              \n",
      "23  01/03/2023  03/03/2023  Pagamento        Vueling Airlines   \n",
      "24                                                              \n",
      "25                                                              \n",
      "\n",
      "                            4         5  \n",
      "0                 Descrizione   Importo  \n",
      "1                                        \n",
      "2            ZURICH BARCELONA    - 1,2€  \n",
      "3      PRIMARK PLAZA CATALUNA            \n",
      "4                                  - 7€  \n",
      "5                   BARCELONA            \n",
      "6        MERCADONA CONSELL DE            \n",
      "7                               - 1,08€  \n",
      "8                 C BARCELONA            \n",
      "9           MERCADONA EL CLOT            \n",
      "10                                - 16€  \n",
      "11                     LLEIDA            \n",
      "12          MERCADONA EL CLOT            \n",
      "13                             - 32,05€  \n",
      "14                     LLEIDA            \n",
      "15          PLUS FRESC LLEIDA      - 5€  \n",
      "16            BARBERIA LLEIDA     - 10€  \n",
      "17   CANONE HYPE PLUS 02/2023            \n",
      "18                                 - 1€  \n",
      "19                                       \n",
      "20      TRENITALIA - LEFRECCE            \n",
      "21                              - 30,6€  \n",
      "22                       ROMA            \n",
      "23  VUELING AIRLINES VUELING.            \n",
      "24                             - 69,99€  \n",
      "25                        COM            \n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "\n",
    "# Carica il file PDF\n",
    "tables = camelot.read_pdf(\"C:/Users/TirocinioMatteo/Desktop/Matteo/Python_3/Vai/ai/PDF/input_doc/6C554FC0-10D0-4655-B8DA-CDF3C2F32FE6.pdf\", pages='all', flavor='stream')\n",
    "# pages='all' per estrarre tutte le pagine\n",
    "# flavor='stream' pr tabelle con bordi non definiti chairamente \n",
    "# flavor='lattice' per tabelle con bordi definiti\n",
    "\n",
    "# Stampa il numero di tabelle trovate\n",
    "print(\"Trovate\", tables.n, \"tabelle\")\n",
    "\n",
    "# Visualizza le tabelle estratte\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"\\nTabella {i + 1}:\")\n",
    "    print(table.df)\n",
    "\n",
    "# Salva le tabelle estratte in formato CSV\n",
    "tables.export('tabelle.csv', f='csv', compress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lettura testo da PDF: pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 02/07/2024\n",
      "Gentile Cliente MATTEO AMAGLIANI,\n",
      "di seguito sono riportati i movimenti effettuati dal 01/12/2023 al 31/12/2023. Questo documento ha solo scopo\n",
      "informativo e non rappresenta un estratto conto.\n",
      "I movimenti dove non è indicata una data contabile sono in attesa di essere contabilizzati. L'importo del\n",
      "pagamento è stato bloccato in seguito alla tua conferma di concludere l'acquisto e deve essere confermato da\n",
      "parte dell'esercente.\n",
      "Lista movimenti\n",
      "dal 01/12/2023 al 31/12/2023\n",
      "Data Data Tipologia Nome Descrizione Importo\n",
      "operazione contabile\n",
      "31/12/2023 03/01/2024 Pagamento pam PAM LOCAL PADOVA - 29€\n",
      "31/12/2023 02/01/2024 Pagamento Aspiag Service S.r.l. ASPIAG SERVICE S.R.L.\n",
      "- 5,97€\n",
      "PADOVA\n",
      "31/12/2023 04/01/2024 Pagamento Trenitalia TRENITALIA - PT WL ROMA - 39,1€\n",
      "Pag. 1 di 5\n",
      "HYPE S.p.A. segreteria@pec.hype.it C.F e P.IVA 02686590023\n",
      "P.zza G. Sella, 1 Capitale sociale: € 1.416.885,00 Iscr Registo Imprese C.C.I.A.A\n",
      "13900 Biella (BI) www.hype.it Monte Rosa Laghi Alto Piemonte REA BI - 205555Data Data Tipologia Nome Descrizione Importo\n",
      "operazione contabile\n",
      "30/12/2023 03/01/2024 Pagamento Mcdonald's MC DONALD'S MONSELICE - 8,1€\n",
      "30/12/2023 03/01/2024 Pagamento Conad CONAD LENDINARA - 13,75€\n",
      "30/12/2023 03/01/2024 Pagamento Conad CONAD LENDINARA - 71,14€\n",
      "28/12/2023 30/12/2023 Pagamento Societa' Agricola Nico SOCIETA' AGRICOLA NICO\n",
      "- 22,2€\n",
      "CHIARAVALLE\n",
      "28/12/2023 30/12/2023 Pagamento Terranova TERRANOVA ANCONA - 26,1€\n",
      "28/12/2023 28/12/2023 Bonifico ordinario Bagaio Sabrina, Caparra per appartamento a\n",
      "+ 690€\n",
      "amagliani Maurizio Padova\n",
      "27/12/2023 29/12/2023 Pagamento Conad CONAD FALCONARA MAR - 28,94€\n",
      "21/12/2023 23/12/2023 Pagamento In's SUPERMERCATO IN'S\n",
      "- 2,68€\n",
      "PADOVA\n",
      "21/12/2023 23/12/2023 Pagamento too Good to go TooGoodToG 4cnqg0kp3vv\n",
      "- 2,99€\n",
      "Copenhagen\n",
      "20/12/2023 22/12/2023 Pagamento Sumup *commercio SumUp *Commercio Bovolone - 16€\n",
      "Pag. 2 di 5\n",
      "HYPE S.p.A. segreteria@pec.hype.it C.F e P.IVA 02686590023\n",
      "P.zza G. Sella, 1 Capitale sociale: € 1.416.885,00 Iscr Registo Imprese C.C.I.A.A\n",
      "13900 Biella (BI) www.hype.it Monte Rosa Laghi Alto Piemonte REA BI - 205555Data Data Tipologia Nome Descrizione Importo\n",
      "operazione contabile\n",
      "19/12/2023 21/12/2023 Pagamento Iper IPER PADOVA PD - 5,56€\n",
      "19/12/2023 19/12/2023 Bonifico ordinario Bagaio Sabrina, BAGAIO SABRINA,\n",
      "amagliani Maurizio AMAGLIANI MAURIZIO\n",
      "UNCRITMM + 600€\n",
      "1101233510085160 URI\n",
      "NOTPROVIDED\n",
      "15/12/2023 17/12/2023 Pagamento la Dama Cafe' Bistrot LA DAMA CAFE' BISTROT\n",
      "- 22,5€\n",
      "ANCONA\n",
      "15/12/2023 17/12/2023 Pagamento la Dama Cafe' Bistrot LA DAMA CAFE' BISTROT\n",
      "- 229€\n",
      "ANCONA\n",
      "15/12/2023 15/12/2023 Bonifico ordinario Bagaio Sabrina, BAGAIO SABRINA,\n",
      "amagliani Maurizio AMAGLIANI MAURIZIO\n",
      "UNCRITMM + 300€\n",
      "1101233481035751 URI\n",
      "NOTPROVIDED\n",
      "15/12/2023 15/12/2023 Bonifico ordinario Bagaio Sabrina, BAGAIO SABRINA,\n",
      "amagliani Maurizio AMAGLIANI MAURIZIO\n",
      "UNCRITMM + 200€\n",
      "1101233480990702 URI\n",
      "NOTPROVIDED\n",
      "14/12/2023 16/12/2023 Pagamento cg Auto di Concettoni. CG AUTO DI CONCETTONI.\n",
      "- 73€\n",
      "FALCONARA MAR\n",
      "Pag. 3 di 5\n",
      "HYPE S.p.A. segreteria@pec.hype.it C.F e P.IVA 02686590023\n",
      "P.zza G. Sella, 1 Capitale sociale: € 1.416.885,00 Iscr Registo Imprese C.C.I.A.A\n",
      "13900 Biella (BI) www.hype.it Monte Rosa Laghi Alto Piemonte REA BI - 205555Data Data Tipologia Nome Descrizione Importo\n",
      "operazione contabile\n",
      "13/12/2023 16/12/2023 Pagamento .blaster Birreria .BLASTER BIRRERIA\n",
      "- 5€\n",
      "MONSANO\n",
      "12/12/2023 12/12/2023 Bonifico ordinario Amagliani Veronica giacca bianca + 130€\n",
      "12/12/2023 14/12/2023 Pagamento Lidl LIDL 1354 FALCONARA MAR - 2,78€\n",
      "11/12/2023 14/12/2023 Pagamento Trenitalia TRENITALIA - PT WL ROMA - 27,9€\n",
      "09/12/2023 12/12/2023 Pagamento dan John DAN JOHN ANCONA AN - 100,79€\n",
      "09/12/2023 11/12/2023 Pagamento Sumup *persicogest sr SumUp *Persicogest sr Colli al\n",
      "- 16€\n",
      "Meta\n",
      "09/12/2023 11/12/2023 Pagamento Sumup *persicogest sr SumUp *Persicogest sr Colli al\n",
      "- 15€\n",
      "Meta\n",
      "08/12/2023 10/12/2023 Pagamento Aspitfano ASPITFANO FANO - 3,1€\n",
      "07/12/2023 09/12/2023 Pagamento Imperial IMPERIAL ANCONA - 130€\n",
      "06/12/2023 06/12/2023 Bonifico ordinario Pro' Leonardo Regalo Angela + 20€\n",
      "Pag. 4 di 5\n",
      "HYPE S.p.A. segreteria@pec.hype.it C.F e P.IVA 02686590023\n",
      "P.zza G. Sella, 1 Capitale sociale: € 1.416.885,00 Iscr Registo Imprese C.C.I.A.A\n",
      "13900 Biella (BI) www.hype.it Monte Rosa Laghi Alto Piemonte REA BI - 205555Data Data Tipologia Nome Descrizione Importo\n",
      "operazione contabile\n",
      "05/12/2023 07/12/2023 Pagamento fsc Food srl FSC FOOD SRL ANCONA - 2,4€\n",
      "05/12/2023 07/12/2023 Pagamento Goldengas spa Falconar GOLDENGAS SPA\n",
      "FALCONAR FALCONARA - 19,85€\n",
      "MAR\n",
      "01/12/2023 01/12/2023 Pagamento Canone Hype Plus 11 CANONE HYPE PLUS 11/2023\n",
      "- 1€\n",
      "/2023\n",
      "Totale Entrate: 1.940 €\n",
      "Totale Uscite: 919,85 €\n",
      "Pag. 5 di 5\n",
      "HYPE S.p.A. segreteria@pec.hype.it C.F e P.IVA 02686590023\n",
      "P.zza G. Sella, 1 Capitale sociale: € 1.416.885,00 Iscr Registo Imprese C.C.I.A.A\n",
      "13900 Biella (BI) www.hype.it Monte Rosa Laghi Alto Piemonte REA BI - 205555\n"
     ]
    }
   ],
   "source": [
    "# Esempio di utilizzo di pdfplumber per estrarre il testo da un PDF\n",
    "# Apri il file PDF\n",
    "with pdfplumber.open(\"C:/Users/TirocinioMatteo/Desktop/Matteo/Python_3/Vai/ai/PDF/input_doc/6E63F9F6-F8D8-44D7-A40B-B7A9BAFDF5B0.pdf\") as pdf:\n",
    "    # Estrai il testo da tutte le pagine\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text()\n",
    "        \n",
    "# Stampa il testo estratto\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codice n.0: (base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Funzione per pulire e strutturare le tabelle estratte\\ndef clean_and_structure_tables(tables):\\n   structured_tables = []\\n   for table in tables:\\n       cleaned_table = []\\n       for row in table:\\n           cleaned_row = [re.sub(r\\'\\\\s+\\', \\' \\', cell.strip()) if cell else \"\" for cell in row]\\n           cleaned_table.append(cleaned_row)\\n       structured_tables.append(cleaned_table)\\n   return structured_tables\\n\\ndef save_data_to_json(data, output_path):\\n   with open(output_path, \\'w\\', encoding=\\'utf-8\\') as json_file:\\n       json.dump(data, json_file, indent=4, ensure_ascii=False)\\n\\n# Richiamare le varie funzioni\\ndef process_financial_document(pdf_path, output_path):\\n   text, tables = extract_text_and_tables_from_pdf(pdf_path)\\n   entities = extract_entities(text)\\n   structured_tables = clean_and_structure_tables(tables)\\n   \\n   # Organizzare i dati finali\\n   data = {\\n       \"text\": text,\\n       \"entities\": entities,\\n       \"tables\": structured_tables\\n   }\\n   save_data_to_json(data, output_path)  '"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # 0 - Estrazione del testo e delle tabelle --> ok anche per multipagina\n",
    "\n",
    " # Funzione per estrarre testo e tabelle dal PDF\n",
    "def extract_text_and_tables_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    tables = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Estrazione del testo\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "            # Estrazione delle tabelle\n",
    "            for table in page.extract_tables():\n",
    "                tables.append(table)\n",
    "    return text.strip(), tables\n",
    "\n",
    "# Funzione per estrarre entità dal testo\n",
    "def extract_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\") \n",
    "    doc = nlp(text)\n",
    "    entities = {}\n",
    "    for ent in doc.ents: # Qui uso NER (Named Entity Recognition)\n",
    "        if ent.label_ not in entities:\n",
    "            entities[ent.label_] = []\n",
    "        entities[ent.label_].append(ent.text)\n",
    "    return entities \n",
    "\n",
    "# Questo modello spacy è in italiano non mi sembra efficiente...\n",
    "# Consultare il sito e fare altri tentativi\n",
    "\"\"\" def extract_entities(text):\n",
    "    nlp = spacy.load(\"it_core_news_sm\")\n",
    "    doc = nlp(text)\n",
    "    entities = {}\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_.lower()\n",
    "        if label not in entities:\n",
    "            entities[label] = []\n",
    "        entities[label].append(ent.text)\n",
    "    return entities \"\"\"\n",
    "\n",
    "# Funzione per pulire e strutturare le tabelle estratte\n",
    "def clean_and_structure_tables(tables):\n",
    "    structured_tables = []\n",
    "    for table in tables:\n",
    "        cleaned_table = []\n",
    "        for row in table:\n",
    "            cleaned_row = [re.sub(r'\\s+', ' ', cell.strip()) if cell else \"\" for cell in row]\n",
    "            cleaned_table.append(cleaned_row)\n",
    "        structured_tables.append(cleaned_table)\n",
    "    return structured_tables\n",
    "\n",
    "def save_data_to_json(data, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Richiamare le varie funzioni\n",
    "def process_financial_document(pdf_path, output_path):\n",
    "    text, tables = extract_text_and_tables_from_pdf(pdf_path)\n",
    "    entities = extract_entities(text)\n",
    "    structured_tables = clean_and_structure_tables(tables)\n",
    "    \n",
    "    # Organizzare i dati finali\n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"entities\": entities,\n",
    "        \"tables\": structured_tables\n",
    "    }\n",
    "    save_data_to_json(data, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codice n.1: (Camelot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Funzione per estrarre testo da PDF --> usa camelot\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "# Funzione per estrarre tabelle dal PDF usando Camelot\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    tables = camelot.read_pdf(pdf_path, pages='all', flavor='stream')  # 'stream' per tabelle senza bordi ben definiti\n",
    "    structured_tables = []\n",
    "    for table in tables:\n",
    "        structured_tables.append(table.df.values.tolist())\n",
    "    return structured_tables\n",
    "\n",
    "# Funzione per estrarre entità dal testo\n",
    "def extract_entities(text):\n",
    "    nlp = spacy.load(\"it_core_news_sm\")\n",
    "    doc = nlp(text)\n",
    "    entities = {\n",
    "        \"organizzazione\": [],\n",
    "        \"luogo\": [],\n",
    "        \"persone\": [],\n",
    "        \"data\": [],\n",
    "        \"valuta\": []\n",
    "    }\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            entities[\"organizzazione\"].append(ent.text)\n",
    "        elif ent.label_ == \"GPE\" or ent.label_ == \"LOC\":\n",
    "            entities[\"luogo\"].append(ent.text)\n",
    "        elif ent.label_ == \"PER\":\n",
    "            entities[\"persone\"].append(ent.text)\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            entities[\"data\"].append(ent.text)\n",
    "        elif ent.label_ == \"MONEY\":\n",
    "            entities[\"valuta\"].append(ent.text)\n",
    "    return entities\n",
    "\n",
    "# Funzione per pulire e strutturare le tabelle estratte\n",
    "def clean_and_structure_tables(tables):\n",
    "    structured_tables = []\n",
    "    for table in tables:\n",
    "        cleaned_table = []\n",
    "        for row in table:\n",
    "            cleaned_row = [re.sub(r'\\s+', ' ', str(cell).strip()) if cell else \"\" for cell in row]\n",
    "            cleaned_table.append(cleaned_row)\n",
    "        structured_tables.append(cleaned_table)\n",
    "    return structured_tables\n",
    "\n",
    "# Funzione per organizzare i movimenti finanziari\n",
    "def organize_movements_table(tables):\n",
    "    movements = {\n",
    "        \"Titolo tabella\": \"Lista movimenti dal 01/07/2024 al 31/07/2024\",\n",
    "        \"Colonne\": [\"Data operazione\", \"Data contabile\", \"Tipologia\", \"Nome\", \"Descrizione\", \"Importo\"],\n",
    "        \"Righe\": [],\n",
    "        \"Totali\": {\n",
    "            \"entrate\": \"\",\n",
    "            \"uscite\": \"\"\n",
    "        }\n",
    "    }\n",
    "    for table in tables:\n",
    "        for row in table:\n",
    "            if \"Totale Entrate\" in row[0]:\n",
    "                movements[\"totals\"][\"entrate\"] = re.findall(r'\\d+[.,]?\\d*', row[0])[0] + \"€\"\n",
    "            elif \"Totale Uscite\" in row[0]:\n",
    "                movements[\"totals\"][\"uscite\"] = re.findall(r'\\d+[.,]?\\d*', row[0])[0] + \"€\"\n",
    "            else:\n",
    "                movements[\"rows\"].append(row)\n",
    "    return movements\n",
    "\n",
    "# Funzione per salvare i dati in formato JSON\n",
    "def save_data_to_json(data, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Funzione principale per processare il documento finanziario\n",
    "def process_financial_document(pdf_path, output_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    raw_tables = extract_tables_from_pdf(pdf_path)\n",
    "    entities = extract_entities(text)\n",
    "    structured_tables = clean_and_structure_tables(raw_tables)\n",
    "    movements_table = organize_movements_table(structured_tables)\n",
    "    \n",
    "    # Organizzare i dati finali\n",
    "    data = {\n",
    "        \"document_text\": text,\n",
    "        \"entities\": entities,\n",
    "        \"tables\": [movements_table]\n",
    "    }\n",
    "    save_data_to_json(data, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codice n.2 (Tesseract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funzione per estrarre testo e tabelle da un'immagine\n",
    "def extract_text_and_tables_from_image(image_path):\n",
    "    text = \"\"\n",
    "    tables = []\n",
    "    img = Image.open(image_path)\n",
    "    # Applica l'OCR con Tesseract sull'immagine\n",
    "    ocr_text = pytesseract.image_to_string(img)\n",
    "    text += ocr_text + \"\\n\"\n",
    "    return text.strip(), tables\n",
    "\n",
    "# Funzione per estrarre testo e tabelle da un PDF\n",
    "def extract_text_and_tables_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    tables = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Estrazione del testo\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "            # Estrazione delle tabelle\n",
    "            for table in page.extract_tables():\n",
    "                tables.append(table)\n",
    "            # Estrazione e OCR delle immagini\n",
    "            for image in page.images:\n",
    "                # Ottieni le coordinate dell'immagine\n",
    "                bbox = (image['x0'], image['top'], image['x1'], image['bottom'])\n",
    "                # Ritaglia l'immagine dalla pagina\n",
    "                img = page.within_bbox(bbox).to_image()\n",
    "                # Applica l'OCR con Tesseract\n",
    "                ocr_text = pytesseract.image_to_string(img)\n",
    "                text += ocr_text + \"\\n\"\n",
    "    return text.strip(), tables\n",
    "\n",
    "# Funzione per estrarre entità dal testo\n",
    "def extract_entities(text, lang='en'):\n",
    "    if lang == 'en':\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    elif lang == 'it':\n",
    "        nlp = spacy.load(\"it_core_news_sm\")\n",
    "    else:\n",
    "        raise ValueError(f\"Lingua non supportata: {lang}\")\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    entities = {}\n",
    "    for ent in doc.ents: # Qui uso NER (Named Entity Recognition)\n",
    "        if ent.label_ not in entities:\n",
    "            entities[ent.label_] = []\n",
    "        entities[ent.label_].append(ent.text)\n",
    "    return entities\n",
    "\n",
    "# Funzione per pulire e strutturare le tabelle estratte\n",
    "def clean_and_structure_tables(tables):\n",
    "    structured_tables = []\n",
    "    for table in tables:\n",
    "        cleaned_table = []\n",
    "        for row in table:\n",
    "            cleaned_row = [re.sub(r'\\s+', ' ', cell.strip()) if cell else \"\" for cell in row]\n",
    "            cleaned_table.append(cleaned_row)\n",
    "    return structured_tables\n",
    "\n",
    "# Funzione per salvare i dati in un file JSON\n",
    "def save_data_to_json(data, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Funzione principale per processare l'input (PDF o immagine) e generare il file JSON di output\n",
    "def process_document(input_path, output_path, lang='en'):\n",
    "    if input_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        # Input è un'immagine\n",
    "        text, tables = extract_text_and_tables_from_image(input_path)\n",
    "    elif input_path.lower().endswith('.pdf'):\n",
    "        # Input è un PDF\n",
    "        text, tables = extract_text_and_tables_from_pdf(input_path)\n",
    "    else:\n",
    "        raise ValueError(\"Il file di input deve essere un'immagine (.png, .jpg, .jpeg, .bmp) o un PDF (.pdf)\")\n",
    "    \n",
    "    entities = extract_entities(text, lang)\n",
    "    structured_tables = clean_and_structure_tables(tables)\n",
    "    \n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"entities\": entities,\n",
    "        \"tables\": structured_tables\n",
    "    }\n",
    "    \n",
    "    save_data_to_json(data, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "PDFSyntaxError",
     "evalue": "No /Root object! - Is this really a PDF?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPDFSyntaxError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/TirocinioMatteo/Desktop/Matteo/Python_3/Vai/ai/PDF/input_doc/Immagine_EstrattoConto_1.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/TirocinioMatteo/Desktop/Matteo/Python_3/Vai/ai/PDF/out_doc/data_pdf_multipagina_2_tesseract.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mprocess_financial_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[62], line 67\u001b[0m, in \u001b[0;36mprocess_financial_document\u001b[1;34m(pdf_path, output_path, lang)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_financial_document\u001b[39m(pdf_path, output_path, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 67\u001b[0m     text, tables \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_and_tables_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     entities \u001b[38;5;241m=\u001b[39m extract_entities(text, lang)\n\u001b[0;32m     69\u001b[0m     structured_tables \u001b[38;5;241m=\u001b[39m clean_and_structure_tables(tables)\n",
      "Cell \u001b[1;32mIn[62], line 12\u001b[0m, in \u001b[0;36mextract_text_and_tables_from_pdf\u001b[1;34m(pdf_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m tables \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpdfplumber\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39mpages:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# Estrazione del testo\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         page_text \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mextract_text()\n",
      "File \u001b[1;32mc:\\Users\\TirocinioMatteo\\.miniconda\\envs\\PDF\\lib\\site-packages\\pdfplumber\\pdf.py:95\u001b[0m, in \u001b[0;36mPDF.open\u001b[1;34m(cls, path_or_fp, pages, laparams, password, strict_metadata, repair, gs_path)\u001b[0m\n\u001b[0;32m     92\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlaparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlaparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_is_external\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_is_external\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PSException:\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream_is_external:\n",
      "File \u001b[1;32mc:\\Users\\TirocinioMatteo\\.miniconda\\envs\\PDF\\lib\\site-packages\\pdfplumber\\pdf.py:45\u001b[0m, in \u001b[0;36mPDF.__init__\u001b[1;34m(self, stream, stream_is_external, path, pages, laparams, password, strict_metadata)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaparams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m laparams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m LAParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlaparams)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword \u001b[38;5;241m=\u001b[39m password\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m \u001b[43mPDFDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPDFParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrsrcmgr \u001b[38;5;241m=\u001b[39m PDFResourceManager()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\TirocinioMatteo\\.miniconda\\envs\\PDF\\lib\\site-packages\\pdfminer\\pdfdocument.py:752\u001b[0m, in \u001b[0;36mPDFDocument.__init__\u001b[1;34m(self, parser, password, caching, fallback)\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 752\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PDFSyntaxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo /Root object! - Is this really a PDF?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LITERAL_CATALOG:\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mSTRICT:\n",
      "\u001b[1;31mPDFSyntaxError\u001b[0m: No /Root object! - Is this really a PDF?"
     ]
    }
   ],
   "source": [
    "# Esempio di utilizzo\n",
    "multi_page_pdf = 'C:/Users/TirocinioMatteo/Desktop/Matteo/Python_3/Vai/ai/PDF/0_documentazione/input_doc/hype_account_balance_pdfs/6C554FC0-10D0-4655-B8DA-CDF3C2F32FE6.pdf'\n",
    "pdf_path = 'C:/Users/TirocinioMatteo/Desktop/Matteo/Python_3/Vai/ai/PDF/0_documentazione/input_doc/hype_account_balance_pdfs/1C579227-CFA7-4297-B47D-1E7F8341514F.pdf'\n",
    "image_path = 'C:/Users/TirocinioMatteo/Desktop/Matteo/Python_3/Vai/ai/PDF/0_documentazione/input_doc/hype_account_balance_images/all_images/1C579227-CFA7-4297-B47D-1E7F8341514F_page-0001.jpg'\n",
    "\n",
    "output_path = 'C:/Users/TirocinioMatteo/Desktop/Matteo/Python_3/Vai/ai/PDF/0_documentazione/out_doc/data_pdf_multipagina_2_tesseract.json'\n",
    "#process_financial_document(image_path, output_path, lang='it')\n",
    "process_financial_document(multi_page_pdf, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
