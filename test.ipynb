{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab74370",
   "metadata": {},
   "source": [
    "- pip install scikit-learn\n",
    "- pip install sentence-transformers\n",
    "- pip install langdetect\n",
    "- pip install spacy\n",
    "- python -m spacy download it_core_news_lg\n",
    "\n",
    "- pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea8a0f",
   "metadata": {},
   "source": [
    "# Primo caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7920a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libreria che consente la rappresentazione vettoriale (embedded) di frasi anziché parole\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Libreria per calcolare la similarità coseno tra vettori (la similiarità coseno misura quanto due vettori sono simili)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Libreria per il rilevamento della lingua del testo\n",
    "from langdetect import detect\n",
    "# Libreria per l'estrazione di entità nominate (Named Entity Recognition - NER)\n",
    "from transformers import pipeline\n",
    "# Libreria per l'elaborazione del linguaggio naturale\n",
    "import spacy\n",
    "# Libreria per creare esempi di addestramento\n",
    "from spacy.training.example import Example \n",
    "# Libreria per mescolare i dati di addestramento\n",
    "import random \n",
    "# Libreria per gestire i percorsi dei file\n",
    "from pathlib import Path \n",
    "\n",
    "# Lista statica di API con descrizioni ed endpoint\n",
    "api_catalog = [\n",
    "    # implementare la seguente api\n",
    "    {\n",
    "        \"descrizione\": \"Crea un nuovo finanziamento con ndg, prodotto, data e convenzione\",\n",
    "        \"endpoint\": \"http://80.88.88.48:8080/accensione-finanziamento\",\n",
    "        \"verbo\": \"GET\", # sarebbe una post, ma andrebbe modificato il backend\n",
    "        \"parametri\": {\"ndg\": \"string\", \"prodotto\": \"string\", \"data\": \"string\", \"convenzione\": \"string\"},\n",
    "    },\n",
    "    # api implementata, ma in corrispondenza dell'url http://80.88.88.48:8080/accensione-finanziamento/elaborazione-primaria\n",
    "    {\n",
    "        \"descrizione\": \"Crea un nuovo finanziamento con ndg, prodotto, data, convenzione, importo e conto corrente\",\n",
    "        \"endpoint\": \"http://80.88.88.48:8080/accensione-finanziamento/elaborazione-primaria\",\n",
    "        \"verbo\": \"POST\",\n",
    "        \"parametri\": {\"ndg\": \"string\", \"prodotto\": \"string\", \"data\": \"string\", \"convenzione\": \"string\", \"importo\": \"string\", \"conto_corrente\": \"string\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "# Carica modello multilingua potente che comprende l'italiano\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# commentato perché questo metodo non produce buoni risultati\n",
    "\"\"\" # Carica pipeline NER per l'italiano\n",
    "nlp = pipeline(\n",
    "    \"ner\", \n",
    "    model=\"DeepMount00/Italian_NER_XXL_v2\", # https://huggingface.co/models\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "def estrai_parametri(testo_input): \n",
    "    entita = nlp(testo_input)\n",
    "    # Esempio di output della pipeline NER:\n",
    "    # Se testo input fosse \"Attiva prestito per l’ndg 998877.\"\n",
    "    # allora l'entita sarebbe [{'entity_group': 'PER', 'word': 'ndg 998877', 'start': 27, 'end': 37, 'score': 0.99}], dove\n",
    "    # - entity_group: il tipo di entità riconosciuta (es. PER per persone, LOC per luoghi, ORG per organizzazioni, MISC per altre entità)\n",
    "    # - word: la parola o frase estratta come entità\n",
    "    # - start: la posizione iniziale dell'entità nel testo\n",
    "    # - end: la posizione finale dell'entità nel testo\n",
    "    # - score: la confidenza del modello nell'aver riconosciuto correttamente l'entità\n",
    "    \n",
    "    risultato = {}\n",
    "    for e in entita:\n",
    "        label = e[\"entity_group\"]\n",
    "        valore = e[\"word\"]\n",
    "        score = e[\"score\"]  # Punteggio di confidenza dell'entità\n",
    "        # Mappa entità generiche a tuoi parametri\n",
    "        if label == \"PER\":  # Persone\n",
    "            risultato.setdefault(\"ndg\", {\"valore\": valore, \"score\": score})\n",
    "        elif label == \"LOC\":  # Luoghi\n",
    "            risultato.setdefault(\"convenzione\", {\"valore\": valore, \"score\": score})\n",
    "        elif label == \"ORG\":  # Organizzazioni\n",
    "            risultato.setdefault(\"prodotto\", {\"valore\": valore, \"score\": score})\n",
    "        elif label == \"MISC\":  # Varie\n",
    "            risultato.setdefault(\"prodotto\", {\"valore\": valore, \"score\": score})\n",
    "        elif label == \"DATE\":\n",
    "            risultato[\"data\"] = {\"valore\": valore, \"score\": score}\n",
    "        elif label == \"NUM\":\n",
    "            if \"importo\" not in risultato:\n",
    "                risultato[\"importo\"] = {\"valore\": valore, \"score\": score}\n",
    "    return risultato \"\"\"\n",
    "\n",
    "def crea_e_addestra_modello_ner():\n",
    "    # Carica il modello italiano pre-addestrato\n",
    "    nlp = spacy.load(\"it_core_news_lg\")\n",
    "    # Test: entità non ancora conosciute\n",
    "    testo = \"Attiva prestito per l’ndg 998877 e tipo mutuo.\"\n",
    "    doc = nlp(testo)\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "    # Definizione degli esempi di addestramento\n",
    "    TRAIN_DATA = [\n",
    "        (\"Attiva prestito per l’ndg 998877\", {\"entities\": [(26, 32, \"NDG\")]}),\n",
    "        (\"Vorrei un mutuo per ndg 123456\", {\"entities\": [(19, 25, \"NDG\"), (10, 15, \"PRODOTTO\")]}),\n",
    "        (\"Mi serve un prestito personale\", {\"entities\": [(13, 32, \"PRODOTTO\")]}),\n",
    "        (\"Accendi finanziamento tipo mutuo per ndg 222333\", {\"entities\": [(28, 33, \"PRODOTTO\"), (38, 44, \"NDG\")]}),\n",
    "        (\"Finanziamento leasing per ndg 445566\", {\"entities\": [(14, 21, \"PRODOTTO\"), (30, 36, \"NDG\")]}),\n",
    "        (\"Avvia pratica mutuo per cliente 999000\", {\"entities\": [(14, 19, \"PRODOTTO\"), (30, 36, \"NDG\")]}),\n",
    "        (\"Stipula prestito per ndg 778899\", {\"entities\": [(8, 16, \"PRODOTTO\"), (21, 27, \"NDG\")]}),\n",
    "        (\"Richiedo leasing per ndg 741852\", {\"entities\": [(9, 16, \"PRODOTTO\"), (21, 27, \"NDG\")]}),\n",
    "    ]\n",
    "    # Aggiungi le nuove label al modello esistente\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    ner.add_label(\"NDG\")\n",
    "    ner.add_label(\"PRODOTTO\")\n",
    "    # Disabilita altri componenti temporaneamente\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    # Addestra il modello con esempi di addestramento\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.resume_training()\n",
    "        for i in range(10):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update([example], drop=0.3, sgd=optimizer)\n",
    "    # Salva il modello addestrato\n",
    "    nlp.to_disk(\"it_ner_model\")\n",
    "\n",
    "def estrai_parametri(testo_input):\n",
    "    nlp = spacy.load(\"it_ner_model\")\n",
    "    doc = nlp(testo_input)\n",
    "    risultato = {}\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_\n",
    "        valore = ent.text.strip()\n",
    "        score = 0.9  # fisso\n",
    "\n",
    "        if label == \"NDG\":\n",
    "            risultato[\"ndg\"] = {\"valore\": valore, \"score\": score}\n",
    "        elif label == \"PRODOTTO\":\n",
    "            risultato[\"prodotto\"] = {\"valore\": valore, \"score\": score}\n",
    "        elif label == \"DATA\":\n",
    "            risultato[\"data\"] = {\"valore\": valore, \"score\": score}\n",
    "        elif label == \"CONVENZIONE\":\n",
    "            risultato[\"convenzione\"] = {\"valore\": valore, \"score\": score}\n",
    "        elif label == \"IMPORTO\":\n",
    "            risultato[\"importo\"] = {\"valore\": valore, \"score\": score}\n",
    "        elif label == \"CONTO\":\n",
    "            risultato[\"conto_corrente\"] = {\"valore\": valore, \"score\": score}\n",
    "    return risultato\n",
    "\n",
    "def aggiorna_modello_ner(testo_input, entita_dict, modello_path=\"it_ner_model\"):\n",
    "    if not Path(modello_path).exists():\n",
    "        raise FileNotFoundError(f\"Modello non trovato in {modello_path}. Assicurati di averlo addestrato e salvato.\")\n",
    "    nlp = spacy.load(\"it_ner_model\")\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # Costruisci le entità con (start, end, label)\n",
    "    entita_corrette = []\n",
    "    for label, valore in entita_dict.items():\n",
    "        start = testo_input.find(valore)\n",
    "        if start == -1:\n",
    "            print(f\"Valore '{valore}' non trovato nel testo. Saltato.\")\n",
    "            continue\n",
    "        end = start + len(valore)\n",
    "        entita_corrette.append((start, end, label))\n",
    "\n",
    "    if not entita_corrette:\n",
    "        print(\"Nessuna entità valida trovata. Nessun aggiornamento.\")\n",
    "        return\n",
    "\n",
    "    doc = nlp.make_doc(testo_input)\n",
    "    example = Example.from_dict(doc, {\"entities\": entita_corrette})\n",
    "    with nlp.select_pipes(enable=[\"ner\"]):\n",
    "        optimizer = nlp.resume_training()\n",
    "        nlp.update([example], sgd=optimizer, drop=0.1)\n",
    "    nlp.to_disk(\"./modello_ner_it\")\n",
    "\n",
    "def get_api(testo_input, soglia_similarita=0.5):\n",
    "    try:\n",
    "        # Verifica che il testo sia in italiano\n",
    "        if detect(testo_input) != \"it\":\n",
    "            return \"Per favore fornisci il testo in italiano.\"\n",
    "\n",
    "        # Calcola embedding del testo utente\n",
    "        embedding_input = model.encode([testo_input])\n",
    "\n",
    "        # Inizializza struttura per il miglior match\n",
    "        migliori_match = {\n",
    "            \"endpoint\": None,\n",
    "            \"score_endpoint\": 0.0,\n",
    "            \"parametri\": {}\n",
    "        }\n",
    "\n",
    "        for api in api_catalog:\n",
    "            # Calcola embedding della descrizione API\n",
    "            embedding_descrizione = model.encode([api[\"descrizione\"]])\n",
    "            # Calcola similarità tra testo utente e descrizione API\n",
    "            sim_descrizione = cosine_similarity(embedding_input, embedding_descrizione)[0][0]\n",
    "\n",
    "            # Estrazione parametri con score individuale\n",
    "            parametri_estratti = estrai_parametri(testo_input)\n",
    "            parametri_finali = {}\n",
    "            for p in api[\"parametri\"]:\n",
    "                if p in parametri_estratti:\n",
    "                    valore = parametri_estratti[p][\"valore\"]\n",
    "                    score = parametri_estratti[p][\"score\"]\n",
    "                    parametri_finali[p] = {\"valore\": valore, \"score\": score}\n",
    "                else:\n",
    "                    parametri_finali[p] = {\"valore\": None, \"score\": 0.0}\n",
    "\n",
    "            # Aggiorna miglior match in base alla similarità descrizione\n",
    "            if sim_descrizione > migliori_match[\"score_endpoint\"]:\n",
    "                migliori_match.update({\n",
    "                    \"endpoint\": api[\"endpoint\"],\n",
    "                    \"score_endpoint\": sim_descrizione,\n",
    "                    \"parametri\": parametri_finali\n",
    "                })\n",
    "\n",
    "        if migliori_match[\"score_endpoint\"] >= soglia_similarita:\n",
    "            return migliori_match\n",
    "        else:\n",
    "            return \"La richiesta non trova corrispondenza con nessuna API. Riprova.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Errore durante l'elaborazione: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdb0f9",
   "metadata": {},
   "source": [
    "# Test primo caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf43d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "esempi_richiesta = [\n",
    "  \"Mi crei un finanziamento con ndg 123456 e tipo mutuo\",\n",
    "  \"Vorrei aprire un mutuo per l’ndg 123456.\",\n",
    "  \"Puoi accendere un finanziamento con codice cliente 654321 e prodotto prestito?\",\n",
    "  \"Avvia una pratica di leasing per il cliente 111222.\",\n",
    "  \"Attiva un prestito per ndg 777888.\",\n",
    "  \"Apri mutuo per il cliente numero 999000.\",\n",
    "  \"Vorrei stipulare un finanziamento mutuo per l’ndg 123999.\",\n",
    "  \"Accendi un prestito per codice cliente 321123.\",\n",
    "  \"Mi serve un leasing per ndg 456789.\",\n",
    "  \"Richiedo un finanziamento tipo mutuo per l’ndg 222333.\",\n",
    "  \"Crea un prestito per cliente 987654.\",\n",
    "  \"Vorrei avviare un mutuo per l’ndg 147258.\",\n",
    "  \"Attiva un nuovo leasing per il cliente 369852.\",\n",
    "  \"Per l’ndg 741852, accendi un finanziamento tipo prestito.\",\n",
    "  \"Avvia mutuo per codice cliente 852963.\",\n",
    "  \"Crea finanziamento prestito per ndg 112233.\",\n",
    "  \"Richiedo leasing con codice cliente 445566.\",\n",
    "  \"Vorrei un finanziamento mutuo per ndg 778899.\",\n",
    "  \"Attiva prestito per l’ndg 998877.\",\n",
    "  \"Apri una pratica di leasing per cliente 112211.\"\n",
    "]\n",
    "\n",
    "crea_e_addestra_modello_ner();\n",
    "\n",
    "for richiesta in esempi_richiesta:\n",
    "    print(f\"  Richiesta utente: {richiesta}\")\n",
    "    print(f\"Endpoint associato: {get_api(richiesta)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad117345",
   "metadata": {},
   "source": [
    "# Correzione modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cf39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testo = \"Attiva un prestito per ndg 777888\"\n",
    "entita = {\n",
    "    \"NDG\": \"777888\",\n",
    "    \"PRODOTTO\": \"prestito\"\n",
    "}\n",
    "aggiorna_modello_ner_smart(testo, entita)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628d17c",
   "metadata": {},
   "source": [
    "# Secondo caso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915649c0",
   "metadata": {},
   "source": [
    "# Importo librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02975895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libreria per l'elaborazione del linguaggio naturale\n",
    "import spacy\n",
    "# Libreria per operazioni numeriche e matriciali\n",
    "import numpy as np\n",
    "# Libreria per il modello di classificazione\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Libreria che consente la rappresentazione vettoriale (embedded) di frasi anziché parole\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# Libreria per calcolare la similarità coseno tra vettori (la similiarità coseno misura quanto due vettori sono simili)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Libreria per modelli di trasformatori (transformers)\n",
    "from transformers import pipeline \n",
    "# Disabilita i warning di transformers\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "# Libreria per gestire tensori e modelli di deep learning\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8802331",
   "metadata": {},
   "source": [
    "# Addestramento machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48469fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\n",
    "    # ==== GET ====\n",
    "    \"cerca\", \"cercami\", \"fai una ricerca\", \"esegui una ricerca\",\n",
    "    \"trova\", \"trovami\", \"rintraccia\", \"identifica\",\n",
    "    \"recupera\", \"recuperami\", \"ottieni dati\", \"estrai informazioni\",\n",
    "    \"mostra\", \"mostrami\", \"fammi vedere\", \"visualizza\",\n",
    "    \"leggi\", \"leggimi\", \"accedi ai dati\", \"ottieni i dati\",\n",
    "    \"visualizza\", \"visualizzami\", \"rendi visibile\", \"presenta\",\n",
    "    \"vedi\", \"vedimi\", \"guarda i dati\", \"dammi una vista\",\n",
    "    \"estrai\", \"estraimi\", \"scarica dati\", \"porta fuori dati\",\n",
    "    \"accedi\", \"accedimi\", \"entra nei dati\", \"consulta\",\n",
    "\n",
    "    # ==== POST ====\n",
    "    \"crea\", \"creami\", \"genera nuovo\", \"costruisci un nuovo\",\n",
    "    \"inserisci\", \"inseriscimi\", \"aggiungi\", \"carica nuovi dati\",\n",
    "    \"richiedi\", \"richiedimi\", \"fai una richiesta\", \"manda una richiesta\",\n",
    "    \"apri\", \"aprimi\", \"avvia una nuova pratica\", \"inizia procedura\",\n",
    "    \"avvia\", \"avviami\", \"dai inizio\", \"comincia processo\",\n",
    "    \"registra\", \"registrami\", \"salva nuovo\", \"archivia dati\",\n",
    "    \"attiva\", \"attivami\", \"metti in funzione\", \"abilita\",\n",
    "    \"compila\", \"compilami\", \"riempi i dati\", \"completa il modulo\",\n",
    "\n",
    "    # ==== PUT ====\n",
    "    \"aggiorna\", \"aggiornami\", \"fai un aggiornamento\", \"modifica con nuovi dati\",\n",
    "    \"modifica\", \"modificami\", \"cambia i dati\", \"rivedi i valori\",\n",
    "    \"correggi\", \"correggimi\", \"sistema dati\", \"risolvi errori\",\n",
    "    \"rivedi\", \"rivedimi\", \"verifica e modifica\", \"ritocca\",\n",
    "    \"sostituisci\", \"sostituiscimi\", \"cambia con altro\", \"scambia contenuto\",\n",
    "    \"ricalcola\", \"ricalcolami\", \"rifai i conti\", \"esegui nuovo calcolo\",\n",
    "\n",
    "    # ==== DELETE ====\n",
    "    \"elimina\", \"eliminami\", \"cancella definitivamente\", \"rimuovi per sempre\",\n",
    "    \"cancella\", \"cancellami\", \"butta via\", \"togli dai dati\",\n",
    "    \"rimuovi\", \"rimuovimi\", \"escludi\", \"levami dai dati\",\n",
    "    \"revoca\", \"revocami\", \"invalida\", \"annulla autorizzazione\",\n",
    "    \"annulla\", \"annullami\", \"ferma l’operazione\", \"interrompi processo\",\n",
    "    \"disattiva\", \"disattivami\", \"spegni\", \"rendi inattivo\"\n",
    "]\n",
    "\n",
    "y_train = [\n",
    "    # ==== GET ====\n",
    "    *[\"GET\"]*36,\n",
    "    # ==== POST ====\n",
    "    *[\"POST\"]*32,\n",
    "    # ==== PUT ====\n",
    "    *[\"PUT\"]*24,\n",
    "    # ==== DELETE ====\n",
    "    *[\"DELETE\"]*24\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900bc43",
   "metadata": {},
   "source": [
    "Opzione 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645fbabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello italiano\n",
    "nlp = spacy.load(\"it_core_news_lg\")\n",
    "\n",
    "#metodo per trasformare x_train che contiene frasi in vettori numerici\n",
    "def vectorizza(frasi):\n",
    "    return np.array([nlp(frase).vector for frase in frasi])\n",
    "\n",
    "# Vettorizza il training set\n",
    "X_vect = vectorizza(X_train)\n",
    "\n",
    "# Definisci il classificatore\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Allena il classificatore\n",
    "clf.fit(X_vect, y_train)\n",
    "\n",
    "# Metodo per estrarre verbo e complemento oggetto da una frase\n",
    "def estrai_verbo_oggetto(frase: str, include_oggetto: bool = False):\n",
    "    # Analizza la frase con spaCy\n",
    "    doc = nlp(frase)\n",
    "    risultati = []\n",
    "    # Itera sui token della frase\n",
    "    for token in doc:\n",
    "        # Verbi\n",
    "        if token.pos_ == \"VERB\":\n",
    "            # Aggiungi il verbo alla lista dei risultati\n",
    "            risultati.append((\"VERBO\", token.text))\n",
    "        # Complemento oggetto (solo se richiesto dal flag)\n",
    "        if include_oggetto and token.dep_ == \"obj\":\n",
    "            # Aggiungi l'oggetto alla lista dei risultati\n",
    "            risultati.append((\"OGGETTO\", token.text))\n",
    "    return risultati\n",
    "\n",
    "def classifica_http(testo_input: str, include_oggetto: bool = False):\n",
    "    # Estraggo le caratteristiche del verbo (e oggetto se richiesto)\n",
    "    caratteristiche_verbo = estrai_verbo_oggetto(testo_input, include_oggetto)\n",
    "    if include_oggetto:\n",
    "        # Estraggo sia la parola del verbo che dell'oggetto (escludendo etichetta e posizioni)\n",
    "        verbo = [v[1] for v in caratteristiche_verbo if v[0] == \"VERBO\" or v[0] == \"OGGETTO\"]\n",
    "    else:\n",
    "        # Estraggo solo la parola del verbo (escludendo etichetta e posizioni)\n",
    "        verbo = [v[1] for v in caratteristiche_verbo if v[0] == \"VERBO\"]\n",
    "    # Vettorizzazione\n",
    "    verbo_vect = vectorizza(verbo)\n",
    "    # Predizione\n",
    "    verbo_predetto = clf.predict(verbo_vect)[0]\n",
    "    # Probabilità per tutte le classi\n",
    "    probs = clf.predict_proba(verbo_vect)[0]   # array 1D tipo [0.05, 0.10, 0.82, 0.03]\n",
    "    # Indice della classe predetta\n",
    "    verbo_predetto_indice = clf.classes_.tolist().index(verbo_predetto)\n",
    "    # Probabilità della classe predetta\n",
    "    score_verbo_predetto = probs[verbo_predetto_indice]\n",
    "    print(\"--- ML\")\n",
    "    print(f\"       Verbo estratto: {verbo}\")\n",
    "    print(f\"  Verbo http predetto: {verbo_predetto}\")\n",
    "    print(f\"               Classi: {clf.classes_}\")\n",
    "    print(f\"          Probabilità: {probs}\")\n",
    "    print(f\"  Indice della classe: {verbo_predetto_indice}\")\n",
    "    print(f\"Probabilità del verbo: {score_verbo_predetto}\")\n",
    "    #esempio\n",
    "    #   Verbo http predetto       → 'POST'\n",
    "    #   Classi                    → ['DELETE', 'GET', 'POST', 'PUT']\n",
    "    #   Probabilità               → [0.05, 0.10, 0.82, 0.03]  # stesso ordine\n",
    "    #   Indice della classe       → 2  (perché 'POST' è il 3° elemento)\n",
    "    #   Probabilità del verbo     → verbo_probabilita[2] = 0.82\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3ea94",
   "metadata": {},
   "source": [
    "Opzione 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modello di embedding\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Calcolo gli embeddings dei tuoi esempi di training\n",
    "X_emb = embedding_model.encode(X_train, convert_to_tensor=True)\n",
    "\"\"\" \n",
    "# print(X_emb)\n",
    "tensor([[ 0.012,  0.034, ..., -0.021],\n",
    "        [ 0.020,  0.040, ..., -0.010],\n",
    "        [ 0.015,  0.028, ..., -0.018],\n",
    "        ...,\n",
    "        [-0.012, 0.033, ..., 0.019]])\n",
    "# print(\"Shape:\", X_emb.shape)\n",
    "Shape: torch.Size([116, 384]) \n",
    "\"\"\"\n",
    "\n",
    "# Zero-shot fallback: \n",
    "# - zero-shot significa che si usa un modello pre-addestrato, che non necessita di addestramento sui tuoi esempi, per classificare i miei esempi\n",
    "# - fallback perché uso prima un metodo principale e se la confidenza è bassa uso questo\n",
    "# In poche parole, serve solo se vuoi gestire verbi/frasi molto fuori dai tuoi esempi\n",
    "\n",
    "# Uso il modello BART di Facebook addestrato su MNLI (Multi-Genre Natural Language Inference)\n",
    "# MNLI è un dataset di inferenza testuale, dove il modello impara a capire se una frase implica, contraddice o è neutrale rispetto a un'altra frase\n",
    "zero_shot_model = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\"\n",
    ")\n",
    "\n",
    "# Etichette HTTP\n",
    "http_labels = [\"GET\", \"POST\", \"PUT\", \"DELETE\"]\n",
    "\n",
    "def classifica_http_2(testo_input: str, include_oggetto: bool = False, top_k: int = 3, fallback_soglia: float = 0.65):\n",
    "    print(\"--- NLP\")\n",
    "    \"\"\" \n",
    "    ESEMPIO\n",
    "    \"butta via i documenti\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # print(\"=== Step 1: Estrazione del verbo dall’input ===\")\n",
    "    caratteristiche_verbo = estrai_verbo_oggetto(testo_input, include_oggetto)\n",
    "    # print(\"       Token estratti:\", caratteristiche_verbo)\n",
    "    if include_oggetto:\n",
    "        # Estraggo sia la parola del verbo che dell'oggetto (escludendo etichetta e posizioni)\n",
    "        verbo = [v[1] for v in caratteristiche_verbo if v[0] == \"VERBO\" or v[0] == \"OGGETTO\"]\n",
    "    else:\n",
    "        # Estraggo solo la parola del verbo (escludendo etichetta e posizioni)\n",
    "        verbo = [v[1] for v in caratteristiche_verbo if v[0] == \"VERBO\"]\n",
    "    # print(\"       Verbo estratto:\", verbo)\n",
    "    \"\"\" \n",
    "    Token estratti: [('VERBO','butta')]\n",
    "    Verbo estratto: ['butta'] \n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"\\n=== Step 2: Calcolo dell’embedding del verbo estratto ===\")\n",
    "    verbo_emb = embedding_model.encode(verbo, convert_to_tensor=True)\n",
    "    # print(\"       Verbo embedded:\", verbo_emb.shape)\n",
    "    \"\"\" \n",
    "    Verbo embedded: (1, 384)\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"\\n=== Step 3: Calcolo della similarità coseno con gli esempi di training ===\")\n",
    "    cos_scores = util.cos_sim(verbo_emb, X_emb)[0] \n",
    "    # print(\"    Similarità coseno:\", cos_scores)\n",
    "    \"\"\" \n",
    "    Similarità coseno: [0.45, 0.12, 0.08, 0.78, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"\\n=== Step 4: Selezione dei top_k esempi più simili ===\")\n",
    "    top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "    preds = [y_train[i] for i in top_results] \n",
    "    verbo_predetto = preds[0] \n",
    "    confidenza = float(cos_scores[top_results[0]])\n",
    "    # print(\"         Indici top_k:\", top_results)\n",
    "    # print(\"     Top_k similarità:\", cos_scores[top_results])\n",
    "    # print(\"         Classi top_k:\", preds)\n",
    "    \"\"\" \n",
    "    Indici top_k: [3, 6, 0]\n",
    "    Valori top_k similarità: [0.78, 0.65, 0.45]\n",
    "    Classi top_k: ['DELETE', 'GET', 'GET']\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"\\n=== Step 5: Predizione basata sulla classe più frequente ===\")\n",
    "    # conta quante volte compare ogni classe nei top_k\n",
    "    counts = {label: preds.count(label) for label in set(preds)} \n",
    "    # perché prendere la classe con il conteggio massimo?\n",
    "    # vantaggio:\n",
    "    # - robustezza contro outlier: se uno dei top_k è un outlier\n",
    "    # svantaggio:\n",
    "    # - non considera la similarità: potrei avere 2 esempi \"GET\" con similarità 0.45 e 0.44, e 1 esempio \"DELETE\" con similarità 0.78\n",
    "    verbo_predetto_2 = max(counts, key=counts.get)\n",
    "    # print(\"     Conteggio classi:\", counts)\n",
    "    # print(\"   Classe più quotata:\", verbo_predetto)\n",
    "    \"\"\" \n",
    "    Conteggio classi: {'DELETE': 1, 'GET': 2}\n",
    "    Classe più quotata: GET\n",
    "    # print(\"Similarità max:\", confidenza)\n",
    "    Confidenza (similarità max): 0.78\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"\\n=== Step 6: Calcolo delle probabilità normalizzate per ogni classe ===\")\n",
    "    probs_dict = {}\n",
    "    for label in http_labels:\n",
    "        # prendo tutti i top_results che hanno questa label\n",
    "        label_indices = [i for i in top_results if y_train[i] == label]\n",
    "        if label_indices:\n",
    "            # converto cos_scores in numpy solo se è un tensor\n",
    "            cos_scores_np = cos_scores.cpu().detach().numpy() if isinstance(cos_scores, torch.Tensor) else cos_scores\n",
    "            # media delle similarità per questa classe\n",
    "            probs_dict[label] = float(np.mean(cos_scores_np[label_indices]))\n",
    "        else:\n",
    "            probs_dict[label] = 0.0\n",
    "    # print(\" Probabilità non norm:\", probs_dict)\n",
    "    # Normalizzo in modo che la somma sia 1\n",
    "    total = sum(probs_dict.values())\n",
    "    if total > 0:\n",
    "        probs = [probs_dict[label]/total for label in http_labels]\n",
    "    else:\n",
    "        # fallback uniforme se total = 0\n",
    "        probs = [1/len(http_labels)]*len(http_labels)\n",
    "    # print(\"     Probabilità norm:\", probs)\n",
    "    \"\"\" \n",
    "    Probabilità non normalizzate: {'GET': 0.285, 'POST': 0.0, 'PUT': 0.0, 'DELETE': 0.78}\n",
    "    Probabilità normalizzate: [0.27, 0.0, 0.0, 0.73]\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"\\n=== Step 7: Fallback zero-shot se la confidenza è bassa ===\")\n",
    "    if confidenza < fallback_soglia:\n",
    "        zero_shot_res = zero_shot_model(testo_input, candidate_labels=http_labels)\n",
    "        verbo_predetto_zero_shot = zero_shot_res['labels'][0]\n",
    "        probs_zero_shot = [zero_shot_res['scores'][zero_shot_res['labels'].index(label)] for label in http_labels]\n",
    "        # print(\"       Caso zero shot\")\n",
    "        # print(\"               Classi:\", zero_shot_res['labels'])\n",
    "        # print(\"          Probabilità:\", zero_shot_res['scores'])\n",
    "        # print(\"        Classe finale:\", verbo_predetto_zero_shot)\n",
    "        if zero_shot_res['scores'][0] > confidenza:\n",
    "            print(\"       Caso zero shot\")\n",
    "            verbo_predetto = verbo_predetto_zero_shot\n",
    "            probs = probs_zero_shot\n",
    "\n",
    "\n",
    "    # print(\"\\n--- Step 8: Risultato finale ---\")\n",
    "    print(\"               Classi:\", http_labels)\n",
    "    print(\"          Probabilità:\", probs)\n",
    "    print(\"      Classe predetta:\", verbo_predetto)\n",
    "    \"\"\" \n",
    "    Classe predetta: GET\n",
    "    Probabilità di tutte le classi: [0.27, 0.0, 0.0, 0.73]\n",
    "    \"\"\"\n",
    "\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e154ba",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo per analizzare il testo e suddividerlo in frasi\n",
    "\"\"\" Il metodo riconosce più frasi separate da punteggiatura, invece, ad esempio, non riconosce invece frasi congiunte da \"e\" o \"poi\" \"\"\"\n",
    "def analizza_frasi(testo: str):\n",
    "    # Analizza la frase con spaCy\n",
    "    doc = nlp(testo)\n",
    "    # Estrai le frasi dal testo\n",
    "    frasi = [sent.text.strip() for sent in doc.sents] \n",
    "    print(f\"       Frasi estratte: {frasi}\")\n",
    "    return len(frasi), frasi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0687a6",
   "metadata": {},
   "source": [
    "# Api da individuare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista statica di API con descrizioni ed endpoint\n",
    "api_catalog = [\n",
    "    {\n",
    "        \"descrizione\": \"Creazione nuovo finanziamento. Creazione per tipo e ndg\",\n",
    "        \"endpoint\": \"api/finanziamento\",\n",
    "        \"verbo_http\": \"POST\"\n",
    "    },\n",
    "    {\n",
    "        \"descrizione\": \"Ricerca un finanziamento esistente. Ricerca per tipo e ndg\",\n",
    "        \"endpoint\": \"api/finanziamento?tipo=mutuo&ndg=123456\",\n",
    "        \"verbo_http\": \"GET\"\n",
    "    },\n",
    "    {\n",
    "        \"descrizione\": \"Cancellazione finanziamento esistente. Cancellazione per ndg\",\n",
    "        \"endpoint\": \"api/finanziamento?ndg=123456\",\n",
    "        \"verbo_http\": \"DELETE\"\n",
    "    },\n",
    "    {\n",
    "        \"descrizione\": \"Aggiornamento finanziamento esistente. Aggiornamento per ndg\",\n",
    "        \"endpoint\": \"api/finanziamento?ndg=123456\",\n",
    "        \"verbo_http\": \"PUT\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f620f",
   "metadata": {},
   "source": [
    "# Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica modello multilingua potente che comprende l'italiano\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def get_api_2(testo_input, tipo_classificatore=\"ML\", include_oggetto=False, ml_weight=0.5, nlp_weight=0.5, soglia_similarita=0.5):\n",
    "    try:\n",
    "        print(f\"     Richiesta utente: {testo_input}\")\n",
    "\n",
    "        # Verifica che la richiesta contenga una sola frase\n",
    "        num_frasi, lista_frasi = analizza_frasi(testo_input)\n",
    "        if num_frasi > 1:\n",
    "            return f\"                       La richiesta contiene {num_frasi} frasi. Per favore invia una sola frase alla volta. Frasi rilevate: {lista_frasi}\"\n",
    "        \n",
    "        # Recupera le probabilità per ogni verbo HTTP\n",
    "        if tipo_classificatore == \"ML\":\n",
    "            probs = classifica_http(testo_input, include_oggetto)\n",
    "        else:\n",
    "            probs = classifica_http_2(testo_input, include_oggetto)\n",
    "\n",
    "        # Calcola embedding del testo utente\n",
    "        embedding_input = model.encode([testo_input])\n",
    "\n",
    "        migliore_match = {\"endpoint\": None, \"verbo_http\": None, \"score\": 0.0}\n",
    "        migliore_match_composito = {\"endpoint\": None, \"verbo_http\": None, \"score\": 0.0}\n",
    "\n",
    "        # Confronta con le descrizioni delle API\n",
    "        for api in api_catalog:\n",
    "\n",
    "            # Calcola embedding della descrizione dell'API\n",
    "            embedding_api = model.encode([api[\"descrizione\"]])\n",
    "\n",
    "            # Calcola la similarità coseno tra l'input e la descrizione dell'API\n",
    "            sim = cosine_similarity(embedding_input, embedding_api)[0][0]\n",
    "\n",
    "            # Aggiorna il miglior match se la similarità è maggiore della soglia\n",
    "            if sim > migliore_match[\"score\"]:\n",
    "                migliore_match = {\"endpoint\": api[\"endpoint\"], \"verbo_http\": api[\"verbo_http\"], \"score\": sim}\n",
    "\n",
    "            # Combina i due score (similarità e probabilità verbo) con pesi alpha e beta\n",
    "            if tipo_classificatore == \"ML\":\n",
    "                verbo_indice = clf.classes_.tolist().index(api[\"verbo_http\"])\n",
    "            else:\n",
    "                verbo_indice = http_labels.index(api[\"verbo_http\"])\n",
    "            score_verbo = probs[verbo_indice]\n",
    "            sim_composito = ml_weight * score_verbo + nlp_weight * sim\n",
    "            if sim_composito > migliore_match_composito[\"score\"]:\n",
    "                migliore_match_composito = {\"endpoint\": api[\"endpoint\"], \"verbo_http\": api[\"verbo_http\"], \"score\": sim_composito}\n",
    "            \n",
    "\n",
    "        print(\"--- NLP 2\")\n",
    "        print(f\"    Similarità coseno: {migliore_match}\")\n",
    "        print(f\"--- {tipo_classificatore} + NLP 2\")\n",
    "\n",
    "        # Controlla se il miglior match supera la soglia di similarità\n",
    "        if migliore_match_composito[\"score\"] >= soglia_similarita:\n",
    "            return f\"                       {migliore_match_composito}\"\n",
    "        else:\n",
    "            return \"                       La richiesta non trova corrispondenza con nessuna API. (score: \" + str(migliore_match_composito[\"score\"]) + \")\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Errore durante l'elaborazione: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97063346",
   "metadata": {},
   "source": [
    "# Test secondo caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bbbab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "esempi_richiesta = [\n",
    "    \"cercami i finanziamenti dell'ndg 123456\",\n",
    "    \"voglio avviare un nuovo finanziamento\",\n",
    "    \"mi cerchi un finanziamento\",\n",
    "    \"ho bisogno di un mutuo per la casa\",\n",
    "    \"vado a comprarmi una pizza\",\n",
    "    \"mi crei un finanziamento con ndg 123456 e tipo mutuo\",\n",
    "]\n",
    "\n",
    "for richiesta in esempi_richiesta:\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(get_api_2(richiesta, tipo_classificatore=\"ML\", include_oggetto=False, ml_weight=0.1, nlp_weight=0.9, soglia_similarita=0.6)),\n",
    "    print(\"----------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bdceee",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Gestione input utente\n",
    "  - Consentire l’inserimento di più frasi in un’unica richiesta, con un sistema che sappia segmentarle e trattarle correttamente.\n",
    "- Classificazione con Machine Learning\n",
    "  - Integrare una libreria per l’estrazione automatica di sinonimi (o termini semanticamente vicini) a partire da una parola, utile per arricchire e generare dataset di training.\n",
    "  - Valutare modelli più potenti rispetto a LogisticRegression (es. SVM, Random Forest, XGBoost, reti neurali leggere).\n",
    "  - Sperimentare diverse parametrizzazioni e strategie di ottimizzazione per addestrare in modo più efficiente il modello scelto.\n",
    "- Classificazione\n",
    "  - Non trattare NLP e ML come due alternative separate, ma sommare i contributi (ensemble) per migliorare la classificazione.\n",
    "- Migliorare l’estrazione dei parametri dopo aver trovato l’API\n",
    "- Apprendimento continuo\n",
    "  - Esplorare meccanismi di aggiornamento “live” del modello, ad esempio con approcci di reinforcement learning o online learning.\n",
    "- Supporto al workflow\n",
    "  - Abilitare l’AI a suggerire le prossime azioni da intraprendere, guidando l’utente nel processo operativo in base al contesto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c01b9",
   "metadata": {},
   "source": [
    "# Terzo caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA\n",
    "\n",
    "# Libreria che consente la rappresentazione vettoriale (embedded) di frasi anziché parole\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Libreria per calcolare la similarità coseno tra vettori (la similiarità coseno misura quanto due vettori sono simili)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Libreria per il rilevamento della lingua del testo\n",
    "from langdetect import detect\n",
    "# Libreria per l'elaborazione del linguaggio naturale\n",
    "import spacy\n",
    "# Libreria per creare esempi di addestramento\n",
    "from spacy.training.example import Example \n",
    "# Libreria per mescolare i dati di addestramento\n",
    "import random \n",
    "# Libreria per gestire i percorsi dei file\n",
    "from pathlib import Path \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Lista statica di API con descrizioni ed endpoint\n",
    "api_catalog = [\n",
    "    # implementare la seguente api\n",
    "    {\n",
    "        \"descrizione\": \"Crea un nuovo finanziamento con ndg, prodotto, data e convenzione\",\n",
    "        \"endpoint\": \"http://80.88.88.48:8080/accensione-finanziamento\",\n",
    "        \"verbo\": \"GET\", # sarebbe una post, ma andrebbe modificato il backend\n",
    "        \"parametri\": {\"ndg\": \"string\", \"prodotto\": \"string\", \"data\": \"string\", \"convenzione\": \"string\"},\n",
    "    },\n",
    "    # api implementata, ma in corrispondenza dell'url http://80.88.88.48:8080/accensione-finanziamento/elaborazione-primaria\n",
    "    {\n",
    "        \"descrizione\": \"Crea un nuovo finanziamento con ndg, prodotto, data, convenzione, importo e conto corrente\",\n",
    "        \"endpoint\": \"http://80.88.88.48:8080/accensione-finanziamento/elaborazione-primaria\",\n",
    "        \"verbo\": \"POST\",\n",
    "        \"parametri\": {\"ndg\": \"string\", \"prodotto\": \"string\", \"data\": \"string\", \"convenzione\": \"string\", \"importo\": \"string\", \"conto_corrente\": \"string\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# ID del modello\n",
    "model_id = \"m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA\"\n",
    "\n",
    "# Inserisci il tuo token personale Hugging Face\n",
    "token = \"hf_xxx\"  # Sostituisci con il tuo token\n",
    "\n",
    "# Carica tokenizer e modello autenticandoti\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, token=token, device_map=\"auto\")\n",
    "\n",
    "# Crea pipeline per generazione testo\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "\n",
    "def estrai_parametri_llm(testo_input, parametri_attesi): \n",
    "    prompt = f\"\"\"\n",
    "    Estrai i seguenti parametri dal testo: {parametri_attesi}.\n",
    "    Rispondi SOLO in JSON con i campi trovati.\n",
    "\n",
    "    Testo: \"{testo_input}\"\n",
    "        \"\"\".strip()\n",
    "    try:\n",
    "        output = generator(prompt, max_new_tokens=300, do_sample=False, temperature=0)[0][\"generated_text\"]\n",
    "        # Estrai solo la parte JSON dal testo generato\n",
    "        start = output.find(\"{\")\n",
    "        end = output.rfind(\"}\") + 1\n",
    "        json_text = output[start:end]\n",
    "        return json.loads(json_text)\n",
    "    except Exception as e:\n",
    "        print(\"Errore nella decodifica:\", e)\n",
    "        return {}\n",
    "\n",
    "def get_api_3(testo_input, soglia_similarita=0.5):\n",
    "    try:\n",
    "        # Verifica che la lingua sia italiana\n",
    "        if detect(testo_input) != \"it\":\n",
    "            return \"Per favore fornisci il testo in italiano.\"\n",
    "\n",
    "        # Calcola embedding del testo utente\n",
    "        embedding_input = model.encode([testo_input])\n",
    "\n",
    "        migliori_match = {\"endpoint\": None, \"score\": 0.0, \"parametri\": {}}\n",
    "\n",
    "        # Confronta con le descrizioni delle API\n",
    "        for api in api_catalog:\n",
    "            # Calcola embedding della descrizione dell'API\n",
    "            embedding_descrizione = model.encode([api[\"descrizione\"]])\n",
    "            # Calcola la similarità coseno tra l'input e la descrizione dell'API\n",
    "            sim = cosine_similarity(embedding_input, embedding_descrizione)[0][0]\n",
    "\n",
    "            # Aggiorna il miglior match se la similarità è maggiore della soglia\n",
    "            if sim > migliori_match[\"score\"]:\n",
    "                migliori_match = {\"endpoint\": api[\"endpoint\"], \"score\": sim, \"parametri\": api[\"parametri\"]}\n",
    "\n",
    "        # Controlla se il miglior match supera la soglia di similarità\n",
    "        if migliori_match[\"score\"] >= soglia_similarita:\n",
    "            # Estrai i parametri richiesti usando LLM\n",
    "            parametri_attesi = migliori_match[\"parametri\"]\n",
    "            parametri_estratti = estrai_parametri_llm(testo_input, parametri_attesi)\n",
    "            migliori_match[\"parametri\"] = parametri_estratti\n",
    "            return f\"{migliori_match['endpoint']}\"\n",
    "        else:\n",
    "            return \"La richiesta non trova corrispondenza con nessuna API. Riprova.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Errore durante l'elaborazione: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4112f580",
   "metadata": {},
   "source": [
    "# Test terzo caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "esempi_richiesta = [\n",
    "  \"Mi crei un finanziamento con ndg 123456 e tipo mutuo\",\n",
    "  \"Vorrei aprire un mutuo per l’ndg 123456.\",\n",
    "  \"Puoi accendere un finanziamento con codice cliente 654321 e prodotto prestito?\",\n",
    "  \"Avvia una pratica di leasing per il cliente 111222.\",\n",
    "  \"Attiva un prestito per ndg 777888.\",\n",
    "  \"Apri mutuo per il cliente numero 999000.\",\n",
    "  \"Vorrei stipulare un finanziamento mutuo per l’ndg 123999.\",\n",
    "  \"Accendi un prestito per codice cliente 321123.\",\n",
    "  \"Mi serve un leasing per ndg 456789.\",\n",
    "  \"Richiedo un finanziamento tipo mutuo per l’ndg 222333.\",\n",
    "  \"Crea un prestito per cliente 987654.\",\n",
    "  \"Vorrei avviare un mutuo per l’ndg 147258.\",\n",
    "  \"Attiva un nuovo leasing per il cliente 369852.\",\n",
    "  \"Per l’ndg 741852, accendi un finanziamento tipo prestito.\",\n",
    "  \"Avvia mutuo per codice cliente 852963.\",\n",
    "  \"Crea finanziamento prestito per ndg 112233.\",\n",
    "  \"Richiedo leasing con codice cliente 445566.\",\n",
    "  \"Vorrei un finanziamento mutuo per ndg 778899.\",\n",
    "  \"Attiva prestito per l’ndg 998877.\",\n",
    "  \"Apri una pratica di leasing per cliente 112211.\"\n",
    "]\n",
    "\n",
    "for richiesta in esempi_richiesta:\n",
    "    print(f\"  Richiesta utente: {richiesta}\")\n",
    "    print(f\"Endpoint associato: {get_api_3(richiesta)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd87fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA\"\n",
    "token = \"hf_xxx\"  # Sostituisci con il tuo token\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, token=token)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Chi sei?\"}]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edoardo.frapiccini\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"m-polignano/ANITA-NEXT-24B-Dolphin-Mistral-UNCENSORED-ITA\",\n",
    "    token = \"hf_xxx\"  # Sostituisci con il tuo token\n",
    "    )\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
